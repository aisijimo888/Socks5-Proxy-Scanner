"""
é›†æˆå¢å¼ºä¸»æ‰«æå™¨
æ•´åˆæ‰€æœ‰æ–°åŠŸèƒ½: æ•°æ®æŒä¹…åŒ–ã€å¢å¼ºè¯„åˆ†ã€æºå¥åº·æ£€æŸ¥ç­‰
"""

import asyncio
import argparse
import logging
import sys
import os
from datetime import datetime

from config import Config
from proxy_sources_fixed import ProxySourceManager
from validators import ProxyValidator
from exporters import ResultExporter
from proxy_database import ProxyDatabase
from enhanced_validator import EnhancedValidator, ProxyScorer
from source_health_checker import SourceHealthChecker
from timezone_utils import get_display_time


async def main():
    """ä¸»å‡½æ•°"""
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='SOCKS5ä»£ç†æ‰«æå™¨ (å¢å¼ºç‰ˆ)')
    parser.add_argument('--timeout', type=int, default=10, help='è¶…æ—¶æ—¶é—´(ç§’)')
    parser.add_argument('--max-concurrency', type=int, default=50, help='æœ€å¤§å¹¶å‘æ•°')
    parser.add_argument('--output', type=str, default='subscribe/proxies.json', help='è¾“å‡ºæ–‡ä»¶')
    parser.add_argument('--log-level', type=str, default='INFO',
                       choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'],
                       help='æ—¥å¿—çº§åˆ«')
    parser.add_argument('--db-path', type=str, default='proxies.db', help='æ•°æ®åº“è·¯å¾„')
    parser.add_argument('--cleanup-days', type=int, default=30, help='æ¸…ç†å¤©æ•°')
    parser.add_argument('--enable-enhanced', action='store_true', 
                       help='å¯ç”¨å¢å¼ºéªŒè¯(DNSæ³„éœ²ã€å¸¦å®½æµ‹è¯•ç­‰)')
    parser.add_argument('--check-sources', action='store_true',
                       help='æ£€æŸ¥ä»£ç†æºå¥åº·çŠ¶å†µ')
    parser.add_argument('--enable-telegram', action='store_true',
                       help='å¯ç”¨Telegram Bot')
    parser.add_argument('--enable-web', action='store_true',
                       help='å¯åŠ¨Web Dashboard')
    parser.add_argument('--enable-blacklist', action='store_true',
                       help='å¯ç”¨ä»£ç†é»‘åå•è¿‡æ»¤')
    parser.add_argument('--auto-blacklist', action='store_true',
                       help='è‡ªåŠ¨å°†æŒç»­å¤±è´¥çš„ä»£ç†åŠ å…¥é»‘åå•')
    parser.add_argument('--blacklist-threshold', type=int, default=5,
                       help='è‡ªåŠ¨åŠ å…¥é»‘åå•çš„å¤±è´¥æ¬¡æ•°é˜ˆå€¼')
    
    args = parser.parse_args()
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(
        level=getattr(logging, args.log_level),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(sys.stdout),
            logging.FileHandler('scanner.log', encoding='utf-8')
        ]
    )
    logger = logging.getLogger(__name__)
    
    logger.info("=" * 70)
    logger.info("SOCKS5ä»£ç†æ‰«æå™¨ (å¢å¼ºç‰ˆ) å¯åŠ¨")
    logger.info("=" * 70)
    
    # åŠ è½½é…ç½®
    config = Config(
        timeout=args.timeout,
        max_concurrency=args.max_concurrency,
        output_file=args.output
    )
    
    
    # åˆå§‹åŒ–æ•°æ®åº“
    db = ProxyDatabase(args.db_path)
    logger.info(f"æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ: {args.db_path}")
    
    # æ£€æŸ¥ä»£ç†æºå¥åº·çŠ¶å†µ(å¯é€‰)
    if args.check_sources:
        logger.info("\nå¼€å§‹æ£€æŸ¥ä»£ç†æºå¥åº·çŠ¶å†µ...")
        source_checker = SourceHealthChecker(timeout=args.timeout)
        source_results = await source_checker.check_all_sources(config.sources)
        report = source_checker.generate_report(source_results)
        print("\n" + report)
        
        # ä¿å­˜æºçŠ¶æ€åˆ°æ•°æ®åº“
        for result in source_results:
            db.update_source_stats(
                result['url'],
                result['is_available'],
                result.get('valid_proxies', 0)
            )
    
    # è·å–ä»£ç†åˆ—è¡¨
    logger.info("\nå¼€å§‹ä»ä»£ç†æºè·å–ä»£ç†åˆ—è¡¨...")
    source_manager = ProxySourceManager(config)
    all_proxies = await source_manager.fetch_all_sources()
    
    if not all_proxies:
        logger.error("âŒ æœªè·å–åˆ°ä»»ä½•ä»£ç†")
        return
    
    logger.info(f"âœ… æˆåŠŸè·å– {len(all_proxies)} ä¸ªä»£ç†")
    
    # é»‘åå•è¿‡æ»¤
    if args.enable_blacklist:
        logger.info("\nåº”ç”¨é»‘åå•è¿‡æ»¤...")
        blacklist = db.get_blacklisted_proxies()
        logger.info(f"   å½“å‰é»‘åå•: {len(blacklist)} ä¸ªä»£ç†")
        
        original_count = len(all_proxies)
        all_proxies = {p for p in all_proxies if p not in blacklist}
        filtered_count = original_count - len(all_proxies)
        
        logger.info(f"   âœ… è¿‡æ»¤æ‰ {filtered_count} ä¸ªé»‘åå•ä»£ç†")
        logger.info(f"   å‰©ä½™ {len(all_proxies)} ä¸ªä»£ç†å¾…éªŒè¯")
    
    # éªŒè¯ä»£ç†
    logger.info("\nå¼€å§‹éªŒè¯ä»£ç†...")
    
    if args.enable_enhanced:
        # ä½¿ç”¨å¢å¼ºéªŒè¯å™¨
        logger.info("ä½¿ç”¨å¢å¼ºéªŒè¯æ¨¡å¼ (åŒ…å«DNSæ³„éœ²ã€å¸¦å®½æµ‹è¯•)")
        validator = EnhancedValidator(timeout=args.timeout)
        valid_results = await validator.validate_batch(
            list(all_proxies),
            max_concurrency=args.max_concurrency
        )
    else:
        # ä½¿ç”¨æ ‡å‡†éªŒè¯å™¨
        validator = ProxyValidator(config)
        valid_results = await validator.validate_proxies(list(all_proxies))
    
    # è¿‡æ»¤æœ‰æ•ˆä»£ç†
    valid_proxies = [r for r in valid_results if r.get('is_valid')]
    logger.info(f"âœ… éªŒè¯å®Œæˆ: {len(valid_proxies)}/{len(all_proxies)} ä¸ªä»£ç†æœ‰æ•ˆ")
    
    # è¯„åˆ†å’Œä¿å­˜åˆ°æ•°æ®åº“
    logger.info("\nè®¡ç®—è¯„åˆ†å¹¶ä¿å­˜åˆ°æ•°æ®åº“...")
    scorer = ProxyScorer(db)
    
    for proxy_data in valid_proxies:
        try:
            # ä¿å­˜ä»£ç†ä¿¡æ¯
            proxy_id = db.save_proxy(proxy_data)
            
            # è·å–å†å²ç»Ÿè®¡
            historical_stats = db.get_proxy_stats(proxy_data['proxy'])
            
            # è®¡ç®—ç»¼åˆè¯„åˆ†
            score = scorer.calculate_score(proxy_data, historical_stats)
            proxy_data['score'] = score
            
            # ä¿å­˜éªŒè¯ç»“æœ
            db.save_validation_result(proxy_data['proxy'], {
                'is_valid': True,
                'response_time': proxy_data.get('response_time'),
                'test_url': proxy_data.get('test_url'),
                'score': score
            })
            
        except Exception as e:
            logger.error(f"å¤„ç†ä»£ç† {proxy_data.get('proxy')} æ—¶å‡ºé”™: {e}")
    
    # ä¿å­˜å¤±è´¥ä»£ç†çš„éªŒè¯è®°å½•ï¼ˆç”¨äºé»‘åå•ç³»ç»Ÿï¼‰
    logger.info("\nä¿å­˜å¤±è´¥ä»£ç†çš„éªŒè¯è®°å½•...")
    failed_count = 0
    for result in valid_results:
        if not result.get('is_valid'):
            try:
                proxy_address = result.get('proxy')
                if proxy_address:
                    # å°è¯•ä¿å­˜ä»£ç†ä¿¡æ¯ï¼ˆå¦‚æœæœ‰åŸºæœ¬ä¿¡æ¯ï¼‰
                    try:
                        db.save_proxy({
                            'proxy': proxy_address,
                            'country': 'Unknown',
                            'country_code': 'UN',
                            'city': 'Unknown'
                        })
                    except:
                        pass  # ä»£ç†å¯èƒ½å·²å­˜åœ¨
                    
                    # ä¿å­˜å¤±è´¥çš„éªŒè¯è®°å½•
                    db.save_validation_result(proxy_address, {
                        'is_valid': False,
                        'response_time': None,
                        'test_url': config.test_urls[0] if config.test_urls else None,
                        'error': result.get('error', 'Validation failed'),
                        'score': 0
                    })
                    failed_count += 1
            except Exception as e:
                logger.debug(f"ä¿å­˜å¤±è´¥è®°å½•æ—¶å‡ºé”™ {proxy_address}: {e}")
    
    logger.info(f"   âœ… å·²ä¿å­˜ {failed_count} ä¸ªå¤±è´¥ä»£ç†çš„éªŒè¯è®°å½•")
    
    # å¯¼å‡ºç»“æœ
    logger.info(f"\nå¯¼å‡ºç»“æœåˆ° {args.output}...")
    exporter = ResultExporter(config)
    await exporter.export_results(valid_proxies)
    
    # è·å–æœ€ä½³ä»£ç†å¹¶é¢å¤–å¯¼å‡º
    logger.info("\nç”Ÿæˆæœ€ä½³ä»£ç†åˆ—è¡¨...")
    best_proxies = db.get_best_proxies(limit=50, min_checks=2, min_success_rate=0.6)
    if best_proxies:
        best_proxies_file = 'subscribe/best_proxies.txt'
        with open(best_proxies_file, 'w', encoding='utf-8') as f:
            for proxy in best_proxies:
                f.write(f"{proxy['proxy_address']}\n")
        logger.info(f"âœ… æœ€ä½³ä»£ç†åˆ—è¡¨å·²ä¿å­˜åˆ° {best_proxies_file} ({len(best_proxies)}ä¸ª)")
    
    # æ•°æ®åº“ç»Ÿè®¡
    logger.info("\næ•°æ®åº“ç»Ÿè®¡:")
    stats = db.get_database_stats()
    logger.info(f"  æ€»ä»£ç†æ•°: {stats['total_proxies']}")
    logger.info(f"  24å°æ—¶æ´»è·ƒ: {stats['active_proxies_24h']}")
    logger.info(f"  24å°æ—¶æˆåŠŸç‡: {stats['success_rate_24h']*100:.1f}%")
    logger.info(f"  æ€»éªŒè¯æ¬¡æ•°: {stats['total_validations']}")
    
    # æ¸…ç†æ—§æ•°æ®
    logger.info(f"\næ¸…ç† {args.cleanup_days} å¤©å‰çš„æ—§æ•°æ®...")
    deleted_validations, deleted_proxies = db.cleanup_old_records(days=args.cleanup_days)
    logger.info(f"âœ… æ¸…ç†å®Œæˆ: åˆ é™¤ {deleted_validations} æ¡éªŒè¯è®°å½•, {deleted_proxies} ä¸ªä»£ç†")
    
    # è‡ªåŠ¨åŠ å…¥é»‘åå•
    if args.auto_blacklist:
        logger.info(f"\næ‰§è¡Œè‡ªåŠ¨é»‘åå•æ£€æŸ¥ (é˜ˆå€¼: {args.blacklist_threshold}æ¬¡å¤±è´¥)...")
        blacklisted_count = db.auto_blacklist_failing_proxies(
            fail_threshold=args.blacklist_threshold,
            days=7
        )
        if blacklisted_count > 0:
            logger.info(f"   âœ… æ–°å¢ {blacklisted_count} ä¸ªä»£ç†åˆ°é»‘åå•")
            
            # æ˜¾ç¤ºé»‘åå•ç»Ÿè®¡
            bl_stats = db.get_blacklist_stats()
            logger.info(f"   é»‘åå•æ€»æ•°: {bl_stats['total_blacklisted']}")
            logger.info(f"   è‡ªåŠ¨æ·»åŠ : {bl_stats['auto_added']}")
            logger.info(f"   æ‰‹åŠ¨æ·»åŠ : {bl_stats['manual_added']}")
        else:
            logger.info(f"   â„¹ï¸ æ²¡æœ‰å‘ç°éœ€è¦åŠ å…¥é»‘åå•çš„ä»£ç†")

    # å¯¼å‡ºé»‘åå•æ–‡ä»¶
    if args.enable_blacklist:
        try:
            blacklist_file = 'subscribe/blacklist.txt'
            all_blacklisted = db.get_blacklisted_proxies()
            if all_blacklisted:
                logger.info(f"\nå¯¼å‡ºé»‘åå•åˆ° {blacklist_file}...")
                with open(blacklist_file, 'w', encoding='utf-8') as f:
                    f.write("# Proxy Blacklist\n")
                    f.write(f"# Total: {len(all_blacklisted)}\n")
                    f.write(f"# Updated: {get_display_time()} (åŒ—äº¬æ—¶é—´)\n\n")
                    for proxy in sorted(all_blacklisted):
                        f.write(f"{proxy}\n")
                logger.info(f"âœ… é»‘åå•å·²ä¿å­˜ ({len(all_blacklisted)}ä¸ª)")
        except Exception as e:
            logger.error(f"å¯¼å‡ºé»‘åå•å¤±è´¥: {e}")
    
    # å¯åŠ¨å¯é€‰åŠŸèƒ½
    # å¯åŠ¨å¯é€‰åŠŸèƒ½
    if args.enable_telegram:
        logger.info("\nå¯åŠ¨Telegram Bot...")
        try:
            from telegram_bot import TelegramBot
            # æ³¨æ„ï¼šæ­¤å¤„ä¸å†ä½¿ç”¨ ConfigManagerï¼Œè€Œæ˜¯æç¤ºç”¨æˆ·å•ç‹¬è¿è¡Œ
            # å¦‚æœéœ€è¦é›†æˆï¼Œåº”ä»ç¯å¢ƒå˜é‡æˆ–argsè¯»å– token
            token = os.getenv("TELEGRAM_BOT_TOKEN")
            if token:
                bot = TelegramBot(token, args.db_path)
                logger.info("âš ï¸  Telegram Botéœ€è¦å•ç‹¬è¿è¡Œ: python telegram_bot.py")
            else:
                logger.warning("âš ï¸  æœªè®¾ç½®TELEGRAM_BOT_TOKEN, è·³è¿‡TelegramåŠŸèƒ½")
        except ImportError:
            logger.warning("âš ï¸  pyTelegramBotAPIæœªå®‰è£…, è·³è¿‡TelegramåŠŸèƒ½")
    
    if args.enable_web:
        logger.info("\nWeb Dashboardå¯å•ç‹¬å¯åŠ¨: python web_dashboard.py")
    
    logger.info("\n" + "=" * 70)
    logger.info("âœ… æ‰«æå®Œæˆ!")
    logger.info("=" * 70)
    
    # æ˜¾ç¤ºè¿è¡Œå»ºè®®
    print("\nğŸ’¡ æç¤º:")
    print("  - æŸ¥çœ‹æœ€ä½³ä»£ç†: cat best_proxies.txt")
    print("  - å¯åŠ¨Web Dashboard: python web_dashboard.py")
    print("  - å¯åŠ¨Telegram Bot: python telegram_bot.py")
    print("  - æŸ¥çœ‹æ•°æ®åº“ç»Ÿè®¡: python -c 'from proxy_database import *; db=ProxyDatabase(); print(db.get_database_stats())'")


if __name__ == '__main__':
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nâš ï¸  ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        logging.error(f"âŒ ç¨‹åºå¼‚å¸¸: {e}", exc_info=True)
        sys.exit(1)
