# -*- coding: utf-8 -*-
# æ–‡ä»¶å: Industrial_Socks5_Engine.py
# æ–°å¢ç‰¹æ€§ï¼šæœ¬åœ°GeoIPè¿‡æ»¤ | Spamhausé»‘åå•æ£€æµ‹ | ç›®æ ‡å›½å®¶ç­›é€‰

import sys
import io
# å¼ºåˆ¶ stdout ä½¿ç”¨ UTF-8 ç¼–ç ï¼ˆé˜²æ­¢ emoji ä¹±ç ï¼‰
if sys.stdout.encoding != 'utf-8':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
# åŒæ—¶å¼ºåˆ¶ stdout å’Œ stderr ä½¿ç”¨ UTF-8
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import asyncio
import aiohttp
from aiohttp_socks import ProxyConnector
import re
import argparse
import os
import dns.resolver  # éœ€è¦ pip install dnspython
import geoip2.database # éœ€è¦ pip install geoip2
import logging
import platform

# ================== é«˜çº§é…ç½® ===================
SOURCES = [
    # ä¸»æµé«˜è´¨é‡æºï¼ˆé«˜è¦†ç›–é¢ï¼‰
    "https://raw.githubusercontent.com/monosans/proxy-list/main/proxies/socks5.txt",
    "https://raw.githubusercontent.com/rdavydov/proxy-list/main/proxies/socks5.txt",
    "https://www.proxy-list.download/api/v1/get?type=socks5",
    "https://raw.githubusercontent.com/clarketm/proxy-list/master/proxy-list-raw.txt",
    
    # CDNåŠ é€Ÿå¤‡ç”¨ï¼ˆç»•è¿‡ GitHub é™æµï¼‰
    "https://cdn.jsdelivr.net/gh/monosans/proxy-list@main/proxies/socks5.txt",
    "https://cdn.jsdelivr.net/gh/rdavydov/proxy-list@main/proxies/socks5.txt",
    "https://cdn.jsdelivr.net/gh/sunny9577/proxy-scraper@main/proxies.txt",
    "https://cdn.jsdelivr.net/gh/ShiftyTR/Proxy-List@master/socks5.txt",
    
    # ä¸­ç­‰çŸ¥ååº¦æºï¼ˆä¸­ç­‰ç«äº‰ï¼‰
    "https://raw.githubusercontent.com/sunny9577/proxy-scraper/master/proxies.txt",
    "https://raw.githubusercontent.com/ShiftyTR/Proxy-List/master/socks5.txt",
    "https://raw.githubusercontent.com/mertguvencli/http-proxy-list/main/socks5.txt",
    "https://raw.githubusercontent.com/zevtyardt/proxy-list/main/socks5.txt",
    
    # ä½ç«äº‰åº¦å°ä¼—æºï¼ˆè¢«çˆ¬å–äººæ•°è¾ƒå°‘ï¼‰
    "https://raw.githubusercontent.com/calclavia/proxy-list/master/proxies.txt",
    "https://raw.githubusercontent.com/opsxcq/proxy-list/master/https.txt",
    "https://raw.githubusercontent.com/almroot/proxylist/master/socks5.txt",
    "https://raw.githubusercontent.com/pteod/proxy-list/main/socks5.txt",
    "https://raw.githubusercontent.com/nanachi-code/proxy-list/main/text/socks5.txt",
    "https://raw.githubusercontent.com/MuRongPIG/Proxy-Master/main/socks5.txt",
    "https://raw.githubusercontent.com/Zaeem20/proxy_scraper/master/proxies.txt",
    "https://raw.githubusercontent.com/officialcedricbahire/proxy-list/main/proxies.txt",
    "https://raw.githubusercontent.com/aslisk/proxylist/main/socks5.txt",
    "https://raw.githubusercontent.com/Anonym0usss/proxy_scraper/main/proxies.txt",
    "https://raw.githubusercontent.com/proxifly/proxylist/main/socks5.txt",
    
    # æå†·é—¨æºï¼ˆæ´»è·ƒä½†å‡ ä¹æ— äººçˆ¬å–ï¼‰
    "https://raw.githubusercontent.com/jetkai/proxy-list/main/online-proxies/txt/socks5.txt",
    "https://raw.githubusercontent.com/roosterkid/openproxylist/master/SOCKS5_RAW.txt",
    "https://raw.githubusercontent.com/QuantumVortex/proxy-list/master/socks5.txt",
    "https://raw.githubusercontent.com/UserR3X/proxy-list/main/socks5.txt",
    "https://raw.githubusercontent.com/Tolar-HashNET/TNCM-Socks5-Proxy-List/main/SOCKS5.txt",
    "https://raw.githubusercontent.com/hookzof/socks5_list/master/proxylist.txt",
    "https://raw.githubusercontent.com/TheSpeedX/SOCKS-LIST/master/socks5.txt",
    "https://raw.githubusercontent.com/vonsylvia/proxy-list/master/socks5.txt",
    "https://raw.githubusercontent.com/yuceltoluyag/Proxy-Lists/master/socks5.txt",
    "https://raw.githubusercontent.com/xiaojingbin/socks5_proxy/main/proxy.txt",
    
    # å›½å†…çˆ¬è™«ç¤¾åŒºæºï¼ˆå›½å†…ç”¨æˆ·å¤šä½†å›½å¤–ç”¨æˆ·å°‘ï¼‰
    "https://raw.githubusercontent.com/jia-sai/proxy-pool/master/proxies.txt",
    "https://raw.githubusercontent.com/Licoy/proxy_pool/main/src/data/proxy.txt",
    "https://raw.githubusercontent.com/Uukenn/ProxyList/master/socks5.txt",
    
    # å¤‡ç”¨ CDN çº¿è·¯ï¼ˆè¶…ä½ç«äº‰ï¼Œä»…å¤‡ç”¨ï¼‰
    "https://cdn.jsdelivr.net/gh/mertguvencli/http-proxy-list@main/socks5.txt",
    "https://cdn.jsdelivr.net/gh/zevtyardt/proxy-list@main/socks5.txt",
]

# å…¬å¼€ä»£ç†ç½‘ç«™åˆ—è¡¨ï¼ˆæ— éœ€æ³¨å†Œï¼Œé€šè¿‡ HTML çˆ¬å– ip:portï¼‰
PUBLIC_PROXY_SITES = [
    {
        "url": "https://www.free-proxy-list.net/",
        "name": "free-proxy-list.net",
        "parser": "table",
    },
    {
        "url": "https://www.socks-proxy.net/",
        "name": "socks-proxy.net",
        "parser": "table",
    },
    {
        "url": "https://free-proxy-list.net/socks5.html",
        "name": "free-proxy-list socks5",
        "parser": "table",
    },
    {
        "url": "https://www.us-proxy.org/",
        "name": "us-proxy.org",
        "parser": "table",
    },
    {
        "url": "https://free-proxy-list.net/uk-proxy.html",
        "name": "free-proxy-list uk",
        "parser": "table",
    },
]

# 1. ç›®æ ‡å›½å®¶ (ç©ºåˆ—è¡¨ä»£è¡¨ä¸é™åˆ¶)
TARGET_COUNTRIES = ["US", "HK", "SG", "JP", "GB"] 

# 2. æœ¬åœ°æ•°æ®åº“è·¯å¾„ (å»ºè®®ä¸‹è½½ GeoLite2-ASN.mmdb ä»¥è·å¾—æè‡´é€Ÿåº¦)
# å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œè¯·å°† Use_Local_DB è®¾ä¸º False
USE_LOCAL_DB = True 
MMDB_PATH = "GeoLite2-ASN.mmdb"

# 3. å¼€å¯ DNSBL é»‘åå•æ£€æµ‹ (è¿‡æ»¤æ‰è¢«æ ‡è®°ä¸ºåƒåœ¾é‚®ä»¶/åƒµå°¸ç½‘ç»œçš„IP)
ENABLE_DNSBL = True
# 4. å¸¸è§çš„æœºæˆ¿ ASN å…³é”®è¯ (æœ¬åœ°åº“è¿‡æ»¤ç”¨)
# å°è¯•ä»å¯é€‰çš„ config.py åŠ è½½è¦†ç›–é¡¹ï¼Œå¦åˆ™ä½¿ç”¨ä¸Šé¢å®šä¹‰çš„é»˜è®¤å€¼
try:
    import config as _config
    BAD_ASN_KEYWORDS = getattr(_config, 'BAD_ASN_KEYWORDS', [])
    TARGET_COUNTRIES = getattr(_config, 'TARGET_COUNTRIES', TARGET_COUNTRIES)
    USE_LOCAL_DB = getattr(_config, 'USE_LOCAL_DB', USE_LOCAL_DB)
    MMDB_PATH = getattr(_config, 'MMDB_PATH', MMDB_PATH)
    ENABLE_DNSBL = getattr(_config, 'ENABLE_DNSBL', ENABLE_DNSBL)
    FOFA_CACHE_FILE = getattr(_config, 'FOFA_CACHE_FILE', 'fofa_cache.txt')
except Exception:
    # config.py å¯é€‰ï¼šæœªæä¾›æ—¶ä½¿ç”¨æ–‡ä»¶é¡¶éƒ¨å®šä¹‰çš„é»˜è®¤å€¼
    BAD_ASN_KEYWORDS = []
    FOFA_CACHE_FILE = 'fofa_cache.txt'

# é»˜è®¤è¯„åˆ†å¥–åŠ±/æƒ©ç½šï¼ˆå¯ç”±å‘½ä»¤è¡Œè¦†ç›–ï¼‰
DEFAULT_PORT_REWARD = 1.0
DEFAULT_COUNTRY_MOBILE_REWARD = 1.8
# è¿‡æ»¤æ¨¡å¼ï¼š'strict' | 'balanced' | 'lenient'
FILTER_MODE = 'strict'

# å‘½ä»¤è¡Œè§£æä¹‹å‰ï¼ŒåŸºäºå¹³å°å†³å®šæ˜¯å¦æ˜¾ç¤º emojiï¼ˆWindows PowerShell 5.1 å¸¸è§ä¹±ç ï¼‰
EMOJI_ENABLED = False if platform.system() == 'Windows' else True

# è¿è¡Œæ—¶å¯è¢«å‘½ä»¤è¡Œè¦†ç›–çš„å˜é‡ï¼ˆå…ˆå®šä¹‰é»˜è®¤å€¼ä»¥é˜² NameErrorï¼‰
PORT_REWARD = DEFAULT_PORT_REWARD
COUNTRY_MOBILE_REWARD = DEFAULT_COUNTRY_MOBILE_REWARD
GLOBAL_SCORE_THRESHOLD = 1.0
# å¹¶å‘æ§åˆ¶ï¼ˆå¯é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–ï¼‰
MAX_CONCURRENCY = 60
# DNSBL é»‘åå•æœåŠ¡åˆ—è¡¨ï¼ˆå¤šæºé™ä½å•ç‚¹æ•…éšœé£é™©ï¼‰
DNSBL_ZONES = [
    'zen.spamhaus.org',
    'b.barracudacentral.org',
    'bl.spamcop.net',
]
# ===============================================

def load_sources_from_file(file_path):
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢å¤–çš„ä»£ç†æºï¼Œæ¯è¡Œä¸€ä¸ªURL"""
    global SOURCES
    if os.path.exists(file_path):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    s = line.strip()
                    if s and s not in SOURCES:
                        SOURCES.append(s)
            logging.info('Loaded extra sources from %s (total sources: %s)', file_path, len(SOURCES))
        except Exception as e:
            logging.warning('Failed to read sources-file %s: %s', file_path, e)
    else:
        logging.warning('sources-file not found: %s', file_path)

class IndustrialCleaner:
    def __init__(self):
        self.raw_proxies = set()
        self.clean_proxies = []
        self.reader = None
        self.fetch_connector = None
        self.logger = logging.getLogger(__name__)
        if USE_LOCAL_DB:
            try:
                self.reader = geoip2.database.Reader(MMDB_PATH)
                print(("âœ… " if EMOJI_ENABLED else "") + "æœ¬åœ° GeoIP æ•°æ®åº“å·²åŠ è½½")
            except Exception as e:
                self.logger.debug('GeoIP load failed: %s', e)
                print(("âš ï¸ " if EMOJI_ENABLED else "") + "æœªæ‰¾åˆ° .mmdb æ–‡ä»¶ï¼Œå·²é™çº§ä¸ºä»…åœ¨çº¿æ£€æµ‹")

    async def fetch_sources(self):
        # å¢å¼ºï¼šæ·»åŠ  User-Agentã€é‡è¯•ã€çŠ¶æ€ç ä¸å†…å®¹é•¿åº¦æ—¥å¿—ï¼Œä¾¿äºè¯Šæ–­â€œ0 ä¸ªåŸå§‹ IPâ€é—®é¢˜
        headers = {"User-Agent": "Mozilla/5.0 (compatible; IndustrialCleaner/1.0)"}
        async with aiohttp.ClientSession(headers=headers, connector=self.fetch_connector) as session:
            for url in SOURCES:
                success = False
                for attempt in range(1, 4):
                    try:
                        async with session.get(url, timeout=10) as resp:
                            status = resp.status
                            text = await resp.text()
                            length = len(text) if text else 0
                            self.logger.info("fetched %s -> status=%s length=%s (attempt %s)", url, status, length, attempt)
                            if status == 200 and length > 0:
                                found = re.findall(r'\b(?:\d{1,3}\.){3}\d{1,3}:\d{2,5}\b', text)
                                self.raw_proxies.update(found)
                                success = True
                                break
                            else:
                                # é 200 æˆ–ç©ºå†…å®¹ï¼Œç­‰å¾…åé‡è¯•
                                await asyncio.sleep(attempt)  # çº¿æ€§ backoff
                    except Exception as e:
                        self.logger.debug("fetch_sources exception for %s (attempt %s): %s", url, attempt, e)
                        await asyncio.sleep(attempt)

                # åˆ é™¤ä»¥ä¸‹é‡å¤æˆ–ä½ä»·å€¼çš„æ—¥å¿—è®°å½•
                # self.logger.warning("failed to fetch or parse %s after retries", url)
        print(("ğŸ“¥ " if EMOJI_ENABLED else "") + f"é‡‡é›†å®Œæˆï¼Œå…± {len(self.raw_proxies)} ä¸ªåŸå§‹ IP")

    async def fetch_from_public_sites(self):
        """
        ä»å…¬å¼€ä»£ç†ç½‘ç«™ï¼ˆæ— éœ€æ³¨å†Œï¼‰æŠ“å– ip:portã€‚
        ä½¿ç”¨ beautifulsoup4 è§£æ HTML è¡¨æ ¼ã€‚
        """
        try:
            from bs4 import BeautifulSoup
        except ImportError:
            self.logger.warning('beautifulsoup4 not installed, skipping public site scraping')
            return

        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
        }
        async with aiohttp.ClientSession(headers=headers, connector=self.fetch_connector) as session:
            for site_config in PUBLIC_PROXY_SITES:
                url = site_config.get('url')
                name = site_config.get('name')
                if not url:
                    continue

                try:
                    async with session.get(url, timeout=15, ssl=False) as resp:
                        if resp.status != 200:
                            self.logger.debug(f'public site {name} returned status {resp.status}')
                            continue

                        html = await resp.text()
                        # å°è¯•ç”¨ BeautifulSoup è§£æ HTML è¡¨æ ¼
                        soup = BeautifulSoup(html, 'html.parser')
                        table = soup.find('table')
                        if not table:
                            # ä¹Ÿå°è¯•æ­£åˆ™æå– ip:port
                            found = re.findall(r"\b(?:\d{1,3}\.){3}\d{1,3}:\d{2,5}\b", html)
                            for ip_port in found:
                                if ip_port not in self.raw_proxies:
                                    self.raw_proxies.add(ip_port)
                            if found:
                                self.logger.info(f'public site {name}: found {len(found)} ips by regex')
                            continue

                        rows = table.find_all('tr')
                        found_count = 0
                        for row in rows[1:]:  # è·³è¿‡è¡¨å¤´
                            cols = row.find_all('td')
                            if len(cols) >= 2:
                                # ç¬¬ä¸€åˆ—é€šå¸¸æ˜¯ IPï¼Œç¬¬äºŒåˆ—æ˜¯ç«¯å£
                                ip = cols[0].text.strip()
                                port_text = cols[1].text.strip()
                                if re.match(r'^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}$', ip) and port_text.isdigit():
                                    ip_port = f"{ip}:{port_text}"
                                    if ip_port not in self.raw_proxies:
                                        self.raw_proxies.add(ip_port)
                                        found_count += 1

                        self.logger.info(f'public site {name}: extracted {found_count} new proxies')
                except Exception as e:
                    self.logger.debug(f'fetch_from_public_sites error for {name}: {e}')
                    continue

    async def is_blacklisted(self, ip):
        """å¼‚æ­¥å¤šDNSBLé»‘åå•æ£€æµ‹ã€‚è½®è¯¢å¤šä¸ªDNSBLåŒºï¼ˆzen.spamhaus.org, barracuda, spamcopï¼‰ä»¥é™ä½æ¼æŠ¥å’Œç½‘ç»œæŠ–åŠ¨ã€‚"""
        if not ENABLE_DNSBL:
            return False
        try:
            reversed_ip = ".".join(reversed(ip.split(".")))
            for zone in DNSBL_ZONES:
                query = f"{reversed_ip}.{zone}"
                try:
                    await asyncio.to_thread(dns.resolver.resolve, query, "A", lifetime=5.0)
                    # ä»»ä¸€åŒºè¿”å›æˆåŠŸï¼ˆæ‰¾åˆ°è®°å½•ï¼‰å³è¡¨ç¤ºè¢«é»‘åå• 
                    self.logger.debug("IP %s blacklisted in %s", ip, zone)
                    return True
                except Exception:
                    # è¯¥åŒºæŸ¥è¯¢å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¸‹ä¸€åŒº
                    continue
            return False
        except Exception as e:
            self.logger.debug("dnsbl check failed for %s: %s", ip, e)
            return False

    def check_local_db(self, ip):
        """æœ¬åœ°æ•°æ®åº“å¿«é€Ÿåˆç­›"""
        if not self.reader: return True # æ²¡åº“é»˜è®¤æ”¾è¡Œ
        try:
            response = self.reader.asn(ip)
            org = response.autonomous_system_organization.lower()
            # æ£€æŸ¥ ASN æ˜¯å¦åŒ…å«æœºæˆ¿å…³é”®è¯
            for kw in BAD_ASN_KEYWORDS:
                if kw in org: return False
            return True
        except Exception as e:
            # åœ¨æœ¬åœ°åº“è§£æå¤±è´¥æ—¶å…è®¸é€šè¿‡ï¼Œä½†è®°å½•è°ƒè¯•ä¿¡æ¯
            if hasattr(self, 'logger'):
                self.logger.debug("check_local_db failed for %s: %s", ip, e)
            return True

    def score_candidate(self, data: dict, ip: str, port: int):
        """å¯¹å€™é€‰èŠ‚ç‚¹æŒ‰å¤šç»´åº¦æ‰“åˆ†å¹¶è¿”å› (score, reasons:list)ã€‚
        è§„åˆ™ï¼ˆå¯è°ƒæ•´ï¼‰:
        - hosting True -> å¼ºåˆ¶æ‹’ç»
        - DNSBL åœ¨ strict æ¨¡å¼ä¸‹å¼ºåˆ¶æ‹’ç»ï¼›åœ¨ balanced ä¸‹æ‰£åˆ†
        - ASN åŒ…å« BAD_ASN_KEYWORDS: strict æ‰£ 3ï¼Œbalanced æ‰£ 1ï¼Œlenient æ‰£ 0
        - mobile True åŠ  COUNTRY_MOBILE_REWARD åˆ†ï¼ˆä½å®…/æ‰‹æœºä¼˜å…ˆï¼‰
        - å›½å®¶åœ¨ TARGET_COUNTRIES åŠ  COUNTRY_MOBILE_REWARD åˆ†
        - å¸¸è§ä»£ç†ç«¯å£(1080,1081,10808,9050,3128,8080) + éšæœºé«˜ç«¯å£(5678,12345,22222ç­‰) åŠ  PORT_REWARD åˆ†
        """
        score = 0.0
        reasons = []

        # hosting -> å¼ºåˆ¶æ‹’ç»
        if data.get('hosting'):
            return -999.0, ['hosting_detected']

        # mobile / country ä½¿ç”¨å¯é…ç½®å¥–åŠ±
        if data.get('mobile'):
            score += COUNTRY_MOBILE_REWARD
            reasons.append('mobile')

        # country
        cc = data.get('countryCode')
        if TARGET_COUNTRIES and cc in TARGET_COUNTRIES:
            score += COUNTRY_MOBILE_REWARD
            reasons.append(f'country_{cc}')

        # ç«¯å£å¥–åŠ±ï¼šåŒ…æ‹¬å¸¸è§ä»£ç†ç«¯å£ + 2025å¹´å¸¸è§çš„éšæœºé«˜ç«¯å£
        # ä¼ ç»Ÿä»£ç†ç«¯å£
        common_proxy_ports = {1080, 1081, 10808, 9050, 3128, 8080}
        # 2025å¹´çœŸå®ä½å®…/æ‰‹æœºSOCKS5å¸¸è§çš„éšæœºé«˜ç«¯å£
        residential_high_ports = {
            5678, 12345, 22222, 33333, 44444, 55555, 
            60080, 9999, 8888, 7777, 6666, 11111,
            2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000,
            10000, 15000, 20000, 25000, 30000, 40000, 50000
        }
        if port in common_proxy_ports or port in residential_high_ports:
            score += PORT_REWARD
            reasons.append(f'port_{port}')

        # ASN penalty
        asn_penalty = 0
        try:
            if self.reader:
                resp = self.reader.asn(ip)
                org = (resp.autonomous_system_organization or '').lower()
                for kw in BAD_ASN_KEYWORDS:
                    if kw in org:
                        if FILTER_MODE == 'strict':
                            asn_penalty = 3
                        elif FILTER_MODE == 'balanced':
                            asn_penalty = 0.5
                        else:
                            asn_penalty = 0
                        reasons.append('bad_asn')
                        break
        except Exception:
            pass
        score -= asn_penalty

        return score, reasons

    async def fetch_ip_info(self, ip, connector):
        """è½®è¯¢å¤šä¸ªIPä¿¡æ¯æä¾›å•†ï¼Œé¡ºåºä¼˜åŒ–ä»¥é¿å…é™æµï¼šip-api > ipinfo > ipapi.co
        æ¯ä¸ªè¯·æ±‚åŠ éšæœºå»¶è¿Ÿ(0.2-0.8s) ä»¥è§„é¿å¹¶å‘é™æµ"""
        import random
        
        providers = [
            ('ip-api', 'http://ip-api.com/json/{ip}?fields=status,countryCode,isp,mobile,hosting,query', 0.5),
            ('ipinfo', 'https://ipinfo.io/{ip}/json', 0.5),
            ('ipapi', 'https://ipapi.co/{ip}/json/', 0.8),  # æ”¾åœ¨æœ€åä»¥è§„é¿å…¶é™æµ
        ]
        
        for prov_name, url_tpl, base_delay in providers:
            # éšæœºå»¶è¿Ÿ + base_delayï¼Œé¿å…å¹¶å‘ä¸€èµ·è¯·æ±‚åŒä¸€æä¾›å•†
            delay = base_delay + random.uniform(0.2, 0.8)
            await asyncio.sleep(delay)
            
            url = url_tpl.format(ip=ip)
            try:
                async with aiohttp.ClientSession(connector=connector, timeout=aiohttp.ClientTimeout(total=10)) as session:
                    async with session.get(url, ssl=False) as resp:
                        if resp.status != 200:
                            continue
                        data = await resp.json()
                        
                        if prov_name == 'ip-api':
                            if data.get('status') == 'success':
                                return data
                        elif prov_name == 'ipinfo':
                            return {
                                'status': 'success',
                                'countryCode': data.get('country'),
                                'isp': data.get('org') or data.get('hostname'),
                                'mobile': False,
                                'hosting': False,
                                'query': ip,
                            }
                        elif prov_name == 'ipapi':
                            return {
                                'status': 'success',
                                'countryCode': data.get('country_code'),
                                'isp': data.get('org') or data.get('asn'),
                                'mobile': data.get('is_vpn', False) or data.get('is_mobile', False),
                                'hosting': data.get('is_datacenter', False),
                                'query': ip,
                            }
            except Exception:
                continue
        return None

    async def test_proxy_connectivity(self, proxy_url, connector, timeout=8):
        """é€šè¿‡ä»£ç†å¯¹å°‘æ•°å¯é çš„è½»é‡ URL å‘èµ·è¯·æ±‚ï¼ŒéªŒè¯ä»£ç†æ˜¯å¦èƒ½è®¿é—®å¤–ç½‘ã€‚
        è¿”å› True è¡¨ç¤ºè‡³å°‘æœ‰ä¸€ä¸ªè¯·æ±‚æˆåŠŸï¼ˆstatus 200/204/301/302ï¼‰ï¼Œå¦åˆ™ Falseã€‚
        """
        test_urls = [
            "http://clients3.google.com/generate_204",
            "https://www.cloudflare.com/cdn-cgi/trace",
            "https://httpbin.org/get",
        ]
        try:
            # ä½¿ç”¨ä¼ å…¥çš„ ProxyConnector å»è¯·æ±‚å¤–éƒ¨ URL
            async with aiohttp.ClientSession(connector=connector, timeout=aiohttp.ClientTimeout(total=timeout)) as session:
                for url in test_urls:
                    try:
                        async with session.get(url, ssl=False) as resp:
                            status = resp.status
                            if status in (200, 204, 301, 302):
                                return True
                    except Exception:
                        # å•ä¸ª URL å¤±è´¥åˆ™å°è¯•ä¸‹ä¸€ä¸ª
                        continue
        except Exception:
            return False
        return False

    async def verify_proxy(self, ip_port):
        ip = ip_port.split(":")[0]
        try:
            port = int(ip_port.split(":")[1])
        except Exception:
            port = 0

        # --- ç¬¬ä¸€é“é˜²çº¿ï¼šæœ¬åœ°æ•°æ®åº“ (æ¯«ç§’çº§) ---
        if USE_LOCAL_DB and not self.check_local_db(ip):
            return None

        # --- ç¬¬ä¸‰é“é˜²çº¿ï¼šçœŸæœºå®æµ‹ (æœ€æ…¢ï¼Œæœ€ååš) ---
        proxy_url = f"socks5://{ip_port}"
        connector = ProxyConnector.from_url(proxy_url, rdns=True)
        
        try:
            # çœŸå®å¤–ç½‘è¿é€šæ€§æµ‹è¯•ï¼ˆå¯é…ç½®å¼ºåº¦ï¼Œé»˜è®¤ç¦ç”¨ä»¥æé«˜é€šè¿‡ç‡ï¼‰
            connectivity_ok = True  # é»˜è®¤è·³è¿‡ï¼Œé™¤éç”¨æˆ·æ˜¾å¼å¯ç”¨
            if getattr(self, 'check_connectivity', False):  # é»˜è®¤ Falseï¼ˆç¦ç”¨ï¼‰
                ok = await self.test_proxy_connectivity(proxy_url, connector, timeout=getattr(self, 'connectivity_timeout', 8))
                if not ok:
                    # strict æ¨¡å¼ç›´æ¥æ‹’ç»ï¼Œbalanced å’Œ lenient æ¨¡å¼æ‰£åˆ†
                    if FILTER_MODE == 'strict':
                        return None
                    # balanced/lenient ç»§ç»­ï¼Œä½†ç¨åä¼šæ‰£åˆ†
                    connectivity_ok = False

            data = await self.fetch_ip_info(ip, connector)
            if not data:
                return None

            # hosting å¼ºæ‹’ç»
            if data.get('hosting'):
                self.logger.info('%s rejected: hosting detected', ip_port)
                return None

            # DNSBL æ£€æŸ¥ï¼ˆå¤šåŒºè½®è¯¢ï¼‰
            is_black = await self.is_blacklisted(ip)
            if is_black and FILTER_MODE == 'strict':
                print(("âš ï¸ " if EMOJI_ENABLED else "") + f"[å‰”é™¤] é»‘åå•IP | {ip}")
                return None

            # æ‰“åˆ†è¯„ä¼°
            score, reasons = self.score_candidate(data, ip, port)

            # DNSBL åœ¨ balanced æ¨¡å¼ä¸‹æ‰£åˆ†
            if is_black and FILTER_MODE == 'balanced':
                score -= 1
                reasons.append('dnsbl')

            # è¿é€šæ€§æµ‹è¯•åœ¨ balanced/lenient æ¨¡å¼ä¸‹æ‰£åˆ†
            if not connectivity_ok and FILTER_MODE in ('balanced', 'lenient'):
                score -= 0.5
                reasons.append('connectivity_weak')

            # æ ¹æ® EMOJI_ENABLED å†³å®šæ˜¯å¦ä½¿ç”¨ emoji
            if EMOJI_ENABLED:
                tag = "ğŸ“± æ‰‹æœº" if data.get("mobile") else "ğŸ  å®¶å®½"
            else:
                tag = "mobile" if data.get("mobile") else "residential"
            cc = data.get('countryCode')

            # å†³ç­–
            try:
                threshold = GLOBAL_SCORE_THRESHOLD
            except NameError:
                threshold = 1.0

            self.logger.info('candidate %s score=%.2f reasons=%s', ip_port, score, reasons)

            if score >= threshold:
                reasons_str = '|'.join(reasons)
                output_line = f"{proxy_url} # {cc}_{tag} | score={score} | reasons={reasons_str}"
                print(("âœ… " if EMOJI_ENABLED else "") + output_line)
                return output_line
            else:
                self.logger.info('%s rejected by score %.2f < %.2f', ip_port, score, threshold)
                return None
        except Exception as e:
            if hasattr(self, 'logger'):
                self.logger.debug("verify_proxy failed for %s: %s", ip_port, e)
            return None

    async def run(self):
        await self.fetch_sources()
        
        sem = asyncio.Semaphore(MAX_CONCURRENCY)
        async def bounded(p):
            async with sem:
                res = await self.verify_proxy(p)
                if res: self.clean_proxies.append(res)

        await asyncio.gather(*[bounded(p) for p in self.raw_proxies])

        if self.clean_proxies:
            with open("Industrial_Socks5.txt", "w", encoding='utf-8') as f:
                f.write("\n".join(self.clean_proxies))
            print(("\nğŸ‰ " if EMOJI_ENABLED else "\n") + f"æ¸…æ´—å®Œæˆï¼è·å¾— {len(self.clean_proxies)} ä¸ªå·¥ä¸šçº§çº¯å‡€èŠ‚ç‚¹")
        else:
            print(("\nâŒ " if EMOJI_ENABLED else "\n") + f"æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„èŠ‚ç‚¹")

if __name__ == "__main__":
    import platform
    if platform.system() == "Windows":
        # Windows ä¸Šéœ€è¦ä½¿ç”¨ SelectorEventLoop æ¥æ”¯æŒ aiodns
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    # é»˜è®¤æ‰“å° INFO çº§åˆ«æ—¥å¿—ï¼Œä¾¿äºæŸ¥çœ‹ fetch_sources çš„çŠ¶æ€
    import logging as _logging
    _logging.basicConfig(level=_logging.INFO, format='%(asctime)s %(levelname)s %(message)s')
    # æ”¯æŒå‘½ä»¤è¡Œé€‰é¡¹ï¼š--lenientï¼ˆå®½æ¾æ¨¡å¼ï¼Œç¦ç”¨ DNSBL/æœ¬åœ° ASN è¿‡æ»¤ï¼‰ï¼Œ
    # ä»¥åŠ --sources-file <path> ä»æœ¬åœ°æ–‡ä»¶åŠ è½½é¢å¤–çš„æºï¼ˆæ¯è¡Œä¸€ä¸ª URLï¼‰
    parser = argparse.ArgumentParser(description='Industrial Socks5 Cleaner')
    parser.add_argument('--lenient', action='store_true', help='Disable DNSBL and local ASN filtering (less strict)')
    parser.add_argument('--filter-mode', choices=['strict','balanced','lenient'], default='strict', help='Filter mode: strict (default), balanced (less strict), lenient (disable several checks)')
    parser.add_argument('--sources-file', type=str, help='Path to a file containing extra sources (one per line)')
    # FOFA å‚æ•°ï¼ˆå¯é€‰ï¼‰ï¼šåœ¨æœ¬åœ°æä¾› email/key åä¼šè°ƒç”¨ FOFA APIï¼Œå¹¶æŠŠç»“æœè¿½åŠ åˆ°å€™é€‰åˆ—è¡¨
    parser.add_argument('--fofa-email', type=str, help='FOFA account email for API access')
    parser.add_argument('--fofa-key', type=str, help='FOFA API key')
    parser.add_argument('--fofa-query', type=str, help='FOFA search query (eg. "title=\"Clash\" && protocol==\"socks5\"")')
    parser.add_argument('--fofa-size', type=int, default=100, help='FOFA page size (max per request)')
    parser.add_argument('--fofa-pages', type=int, default=1, help='Maximum FOFA pages to fetch')
    parser.add_argument('--fofa-force', action='store_true', help='Force refresh FOFA cache (ignore local cache)')
    parser.add_argument('--fetch-proxy', type=str, default="http://127.0.0.1:62809", help='Optional proxy URL to use for fetching sources')
    parser.add_argument('--port-reward', type=float, help='Reward for common proxy ports (overrides default)')
    parser.add_argument('--country-mobile-reward', type=float, help='Reward for country/mobile match (overrides default)')
    parser.add_argument('--score-threshold', type=float, help='Global score threshold to accept a candidate (overrides default per filter-mode)')
    parser.add_argument('--emoji', action='store_true', help='Force-enable emoji output (overrides platform default)')
    parser.add_argument('--no-emoji', action='store_true', help='Disable emoji output')
    parser.add_argument('--concurrency', type=int, help='Max concurrent proxy tests (default 100)')
    parser.add_argument('--connectivity', action='store_true', help='Enable heavyweight connectivity tests (HTTP via proxy) - disabled by default')
    parser.add_argument('--connectivity-timeout', type=int, default=8, help='Timeout seconds for connectivity test requests (default 8)')
    parser.add_argument('--quick', action='store_true', help='Quick mode: use first 3 sources only, skip public sites, lower concurrency')
    parser.add_argument('--no-public', action='store_true', help='Skip public proxy site scraping')
    args = parser.parse_args()

    # CLI override for emoji display: explicit flags win
    if getattr(args, 'emoji', False):
        EMOJI_ENABLED = True
    if getattr(args, 'no_emoji', False):
        EMOJI_ENABLED = False

    # å¹¶å‘æ§åˆ¶
    if args.concurrency:
        MAX_CONCURRENCY = args.concurrency
        _logging.info('Concurrency set to %d', MAX_CONCURRENCY)
    
    # --quick æ¨¡å¼ï¼šå¿«é€Ÿæµ‹è¯•ï¼Œä»…ç”¨å‰ 3 ä¸ªæºï¼Œé™ä½å¹¶å‘ï¼Œè·³è¿‡å…¬å¼€ç«™ç‚¹
    if args.quick:
        SOURCES[:] = SOURCES[:3]
        MAX_CONCURRENCY = min(MAX_CONCURRENCY, 20)
        _logging.info('Quick mode: using first 3 sources, concurrency=%d', MAX_CONCURRENCY)
        args.no_public = True

    # å¦‚æœæä¾›äº†æœ¬åœ° sources æ–‡ä»¶ï¼ŒåŠ è½½å¹¶è¿½åŠ åˆ° SOURCES
    if args.sources_file:
        load_sources_from_file(os.path.abspath(args.sources_file))

    # å¤šæºé‡‡é›† orchestratorï¼šå…ˆæŠ“å…¬å¼€ç½‘ç«™ï¼Œå† FOFAï¼Œæœ€åæ‰§è¡Œå¸¸è§„æ£€æµ‹
    async def _orchestrator():
        cleaner = IndustrialCleaner()

        # å¦‚æœç”¨æˆ·æä¾›äº† fetch-proxyï¼Œåœ¨ cleaner ä¸Šè®¾ç½®å¯¹åº”çš„ connector
        if args.fetch_proxy:
            try:
                cleaner.fetch_connector = ProxyConnector.from_url(args.fetch_proxy)
                _logging.info('Configured fetch proxy: %s', args.fetch_proxy)
            except Exception as e:
                _logging.warning('Failed to create fetch connector from %s: %s', args.fetch_proxy, e)

        # è¿é€šæ€§æ£€æµ‹æ§åˆ¶ï¼šé»˜è®¤ç¦ç”¨ï¼ˆå¤ªä¸¥æ ¼ï¼‰ï¼Œç”¨æˆ·å¯é€šè¿‡ --connectivity æ˜¾å¼å¯ç”¨
        cleaner.check_connectivity = getattr(args, 'connectivity', False)  # é»˜è®¤ False
        cleaner.connectivity_timeout = getattr(args, 'connectivity_timeout', 8)

        # ç¬¬ä¸€æ­¥ï¼šä»å…¬å¼€ç½‘ç«™æŠ“å–ï¼ˆæ— éœ€å‡­è¯ï¼‰
        if not args.no_public:
            _logging.info('Fetching from public proxy sites (no auth required)...')
            await cleaner.fetch_from_public_sites()
        else:
            _logging.info('Skipping public proxy sites (--no-public or --quick mode)')

        # ç¬¬äºŒæ­¥ï¼šå¦‚æœæŒ‡å®šäº† FOFA å‚æ•°ï¼Œä¼˜å…ˆå°è¯•ä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œé™¤éç”¨æˆ·å¼ºåˆ¶åˆ·æ–°
        if args.fofa_key and args.fofa_email and args.fofa_query:
            _logging.info('FOFA fetch requested (using local cache unless --fofa-force provided)')
            try:
                if os.path.exists(FOFA_CACHE_FILE) and not args.fofa_force:
                    # ä»æœ¬åœ° cache åŠ è½½ï¼ˆæŒ‰è¡Œï¼‰ï¼Œå¹¶åŠ å…¥å€™é€‰é›†åˆ
                    with open(FOFA_CACHE_FILE, 'r', encoding='utf-8') as cf:
                        cached = [l.strip() for l in cf if l.strip()]
                    cleaner.raw_proxies.update(cached)
                    _logging.info('Loaded %s entries from FOFA cache (%s)', len(cached), FOFA_CACHE_FILE)
                else:
                    _logging.info('Fetching from FOFA API (this may consume your FOFA quota)')
                    found = await cleaner.fetch_from_fofa(args.fofa_email, args.fofa_key, args.fofa_query, size=args.fofa_size, max_pages=args.fofa_pages)
                    # å°†æ–°æ‰¾åˆ°çš„ç»“æœå†™å…¥æœ¬åœ°ç¼“å­˜ä»¥ä¾¿æœªæ¥ä½¿ç”¨ï¼ˆè¦†ç›–ï¼‰
                    try:
                        if found:
                            with open(FOFA_CACHE_FILE, 'w', encoding='utf-8') as cf:
                                cf.write('\n'.join(sorted(found)))
                            _logging.info('Wrote %s FOFA results to cache %s', len(found), FOFA_CACHE_FILE)
                        else:
                            _logging.info('FOFA returned no new results; cache not updated')
                    except Exception as e:
                        _logging.warning('Failed to write FOFA cache: %s', e)
            except Exception as e:
                _logging.warning('FOFA cache/fetch handling failed: %s', e)
        
        # ç¬¬ä¸‰æ­¥ï¼šç»§ç»­å¸¸è§„é‡‡é›†ä¸æ£€æµ‹æµç¨‹
        await cleaner.run()

    # å®½æ¾æ¨¡å¼ï¼šç¦ç”¨é»‘åå•ä¸æœ¬åœ° ASN è¿‡æ»¤ï¼Œé¿å…è¿‡åº¦å‰”é™¤
    # æ”¯æŒä¸‰ç§è¿‡æ»¤æ¨¡å¼ï¼šstrict (é»˜è®¤) / balanced / lenient
    FILTER_MODE = args.filter_mode
    if args.filter_mode == 'lenient' or args.lenient:
        ENABLE_DNSBL = False
        USE_LOCAL_DB = False
        BAD_ASN_KEYWORDS = []
        _logging.info('Lenient mode enabled: DNSBL and local ASN checks disabled')

    # å…¨å±€è¯„åˆ†é˜ˆå€¼ï¼šå¦‚æœç”¨æˆ·æä¾›åˆ™è¦†ç›–é»˜è®¤ï¼›å¦åˆ™æŒ‰ filter-mode é€‰æ‹©é»˜è®¤å€¼
    if args.score_threshold is not None:
        GLOBAL_SCORE_THRESHOLD = float(args.score_threshold)
    else:
        if FILTER_MODE == 'strict':
            GLOBAL_SCORE_THRESHOLD = 0.0  # ä¸¥æ ¼æ¨¡å¼ï¼šæ¥å—æ‰€æœ‰é€šè¿‡ IP éªŒè¯çš„èŠ‚ç‚¹ï¼ˆåŸºçº¿ï¼‰
        elif FILTER_MODE == 'balanced':
            GLOBAL_SCORE_THRESHOLD = 0.0  # å¹³è¡¡æ¨¡å¼ï¼šæ¥å—æ‰€æœ‰é€šè¿‡ IP éªŒè¯çš„èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬åŸºç¡€åˆ† 0.5 å‡æ‰£åï¼‰
        else:
            GLOBAL_SCORE_THRESHOLD = -999.0  # å®½æ¾æ¨¡å¼ï¼šå‡ ä¹æ¥å—æ‰€æœ‰
    _logging.info('Global score threshold set to %.2f (mode=%s)', GLOBAL_SCORE_THRESHOLD, FILTER_MODE)

    # å¥–åŠ±å‚æ•°ï¼šå…è®¸ç”¨æˆ·é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–é»˜è®¤å¥–åŠ±
    if args.port_reward is not None:
        PORT_REWARD = float(args.port_reward)
    else:
        PORT_REWARD = 1.0  # 2025å¹´ä¼˜åŒ–é»˜è®¤å€¼

    if args.country_mobile_reward is not None:
        COUNTRY_MOBILE_REWARD = float(args.country_mobile_reward)
    else:
        COUNTRY_MOBILE_REWARD = 1.8  # 2025å¹´ä¼˜åŒ–é»˜è®¤å€¼

    _logging.info('PORT_REWARD=%.2f COUNTRY_MOBILE_REWARD=%.2f', PORT_REWARD, COUNTRY_MOBILE_REWARD)

    asyncio.run(_orchestrator())